# 模型微调云服务平台需求说明文档

## 1. 项目概述

### 1.1 项目背景
本项目旨在构建一个基于 LlamaFactory 的模型微调云服务平台，通过 Web 界面让用户可以方便地提交和管理模型微调任务。后端通过 SSH 连接到云服务器上的 LlamaFactory 环境执行微调任务。

### 1.2 技术栈
- **前端**: Vite + React/Vue3（推荐 React） + TypeScript
- **后端**: Python 3.8+ + FastAPI + Paramiko（SSH 客户端）+ SQLite（用户数据存储）
- **认证**: JWT Token（JSON Web Token）
- **文件存储**: 远程服务器文件系统（通过 SSH）
- **远程服务**: LlamaFactory（通过 SSH 连接执行）

### 1.3 最小功能单元目标
实现最基础的功能，确保能够跑通完整的流程，不追求工业级的完善度。

## 2. 功能需求

### 2.1 用户认证模块（优先级：最高）

#### 2.1.1 用户注册
**功能描述**: 新用户注册账户

**前端需求**:
- 注册表单，包含以下字段：
  - 用户名（必填，字符串，3-20字符）
  - 邮箱（必填，邮箱格式验证）
  - 密码（必填，至少6位）
  - 确认密码（必填，需与密码一致）
- 表单验证（前端基础验证）
- 提交按钮
- 注册成功后自动跳转到登录页

**后端需求**:
- REST API 端点：`POST /api/auth/register`
- 验证用户输入（用户名唯一性、邮箱格式、密码强度）
- 密码加密存储（使用 bcrypt 或类似库）
- 用户信息存储到 SQLite 数据库
- 返回成功或错误信息

#### 2.1.2 用户登录
**功能描述**: 用户登录获取访问令牌

**前端需求**:
- 登录表单，包含以下字段：
  - 用户名或邮箱（必填）
  - 密码（必填）
- 登录按钮
- 登录成功后保存 Token 到 localStorage
- 登录成功后跳转到首页
- "记住我"选项（可选，延长 Token 有效期）

**后端需求**:
- REST API 端点：`POST /api/auth/login`
- 验证用户名/邮箱和密码
- 生成 JWT Token
- 返回 Token 和用户基本信息
- Token 有效期：24小时（或根据"记住我"选项调整）

#### 2.1.3 用户状态管理
**功能描述**: 前端维护用户登录状态

**前端需求**:
- 登录状态检查（从 localStorage 读取 Token）
- Token 过期自动退出登录
- 用户信息展示（导航栏显示用户名）
- 退出登录功能（清除 Token，跳转到登录页）
- 路由保护（未登录用户重定向到登录页）

**后端需求**:
- REST API 端点：`GET /api/auth/me`（获取当前用户信息，需要 Token 验证）
- Token 验证中间件
- 返回当前登录用户信息

### 2.2 文件存储模块（优先级：高）

#### 2.2.1 数据集文件管理
**功能描述**: 用户上传和管理训练数据集文件

**前端需求**:
- 数据集文件列表页面
- 文件上传功能：
  - 文件选择器（支持 JSON/JSONL 等格式）
  - 上传进度显示
  - 文件大小限制提示（如：最大 100MB）
- 文件列表显示：
  - 文件名
  - 文件大小
  - 上传时间
  - 操作按钮（删除、下载、使用）
- 文件使用：点击"使用"按钮，将文件路径填充到任务提交表单

**后端需求**:
- REST API 端点：
  - `POST /api/files/datasets`（上传数据集文件）
  - `GET /api/files/datasets`（获取用户的数据集文件列表）
  - `DELETE /api/files/datasets/{file_id}`（删除文件）
  - `GET /api/files/datasets/{file_id}/download`（下载文件）
- 文件存储路径：`/remote/path/users/{user_id}/datasets/{filename}`
- 通过 SSH 上传文件到远程服务器
- 文件信息存储到数据库（文件名、路径、大小、用户ID、上传时间）
- 用户只能访问自己的文件

#### 2.2.2 模型权重文件管理
**功能描述**: 管理训练完成的模型权重文件

**前端需求**:
- 模型权重文件列表页面
- 列表显示：
  - 模型名称/任务名称
  - 模型路径
  - 文件大小
  - 创建时间（训练完成时间）
  - 状态（训练中/已完成）
  - 操作按钮（查看详情、下载、删除、用于对话）
- 从训练任务自动添加：训练完成后，自动将模型路径添加到模型列表
- "用于对话"按钮：选择该模型进行对话

**后端需求**:
- REST API 端点：
  - `GET /api/files/models`（获取用户的模型权重文件列表）
  - `POST /api/files/models`（手动添加模型路径，或从任务自动添加）
  - `DELETE /api/files/models/{model_id}`（删除模型记录，不删除实际文件）
  - `GET /api/files/models/{model_id}`（获取模型详情）
- 模型信息存储到数据库（模型名称、路径、任务ID、用户ID、创建时间）
- 模型路径来自训练任务的输出目录
- 用户只能访问自己的模型

### 2.3 对话模块（优先级：高）

#### 2.3.1 模型选择
**功能描述**: 选择已训练的模型进行对话

**前端需求**:
- 对话页面顶部显示模型选择器（下拉选择）
- 显示当前选择的模型名称和路径
- 模型列表从"我的模型"页面获取
- 切换模型时提示用户确认（如果有未发送的消息）

**后端需求**:
- REST API 端点：`GET /api/models/available`（获取用户可用的模型列表）
- 返回模型列表（模型ID、名称、路径）
- 验证模型路径是否存在（通过 SSH 检查）

#### 2.3.2 对话交互
**功能描述**: 与选定的模型进行对话

**前端需求**:
- 对话界面：
  - 消息历史显示区域（滚动显示）
  - 用户消息显示在右侧
  - AI 回复显示在左侧
  - 输入框（支持多行文本）
  - 发送按钮
  - 清空对话按钮
- 消息发送：
  - 点击发送按钮或按 Enter 键发送
  - 显示发送状态（发送中...）
  - 显示 AI 回复加载状态
- 消息历史：
  - 本地保存对话历史（localStorage 或 IndexedDB）
  - 支持清空当前对话

**后端需求**:
- REST API 端点：`POST /api/chat/completion`
- 请求参数：
  - `model_path`: 模型路径（LoRA 适配器路径）
  - `base_model_path`: 基础模型路径（可选，从模型信息中获取）
  - `messages`: 对话消息列表（格式：[{"role": "user", "content": "..."}, ...]）
  - `temperature`: 温度参数（可选，默认 0.7）
  - `max_tokens`: 最大生成 tokens（可选，默认 2048）
- 通过 SSH 执行 Python 脚本进行推理（非交互式）
- 实现方式：将消息序列化为 JSON，通过 SSH 执行 Python 脚本
- 脚本使用 LlamaFactory 的 Python API 进行推理
- 一次性返回完整回复（最小实现不支持流式输出）
- 返回格式：`{"role": "assistant", "content": "..."}`

### 2.4 核心功能模块

#### 2.4.1 任务提交模块（优先级：最高）
**功能描述**: 用户通过 Web 界面提交模型微调任务

**前端需求**:
- 提供任务提交表单，包含以下字段：
  - 任务名称（必填，字符串）
  - 基础模型名称（必填，下拉选择，如：qwen/Qwen2-7B-Instruct）
  - 数据集选择（必填，下拉选择，从"我的数据集"中选择，或手动输入路径）
  - 训练轮数（可选，默认值：3，整数）
  - 学习率（可选，默认值：5e-5，浮点数）
  - 批次大小（可选，默认值：4，整数）
  - 输出目录（可选，字符串，默认：./saves/{user_id}/{model_name}/lora/sft）
- 表单验证（前端基础验证）
- 提交按钮
- 提交成功后显示任务 ID
- 需要用户登录才能提交

**后端需求**:
- REST API 端点：`POST /api/tasks`（需要 Token 验证）
- 从 Token 中获取用户 ID
- 接收任务参数，验证必填字段
- 验证数据集文件是否存在且属于当前用户
- 生成唯一任务 ID（UUID）
- 将任务信息存储到数据库（关联用户ID）
- 输出目录使用用户专属路径：`/remote/path/users/{user_id}/models/{task_id}`
- 返回任务 ID 和状态

**SSH 执行需求**:
- 通过 Paramiko 连接到远程服务器
- 构建 LlamaFactory 训练命令（示例格式）：
  ```bash
  llamafactory-cli train \
    --model_name_or_path {model_name} \
    --dataset {dataset_path} \
    --template default \
    --finetuning_type lora \
    --output_dir {output_dir} \
    --num_train_epochs {epochs} \
    --learning_rate {learning_rate} \
    --per_device_train_batch_size {batch_size} \
    --cutoff_len 1024 \
    --plot_loss
  ```
- 在后台异步执行命令（使用 nohup 或 screen）
- 将任务状态存储为 "running"

#### 2.4.2 任务列表模块（优先级：高）
**功能描述**: 显示所有提交的任务及其状态

**前端需求**:
- 任务列表页面，显示：
  - 任务 ID
  - 任务名称
  - 模型名称
  - 状态（pending/running/completed/failed）
  - 创建时间
  - 操作按钮（查看详情/查看日志）
- 列表支持刷新功能
- 简单的状态标识（颜色区分）

**后端需求**:
- REST API 端点：`GET /api/tasks`（需要 Token 验证）
- 从 Token 中获取用户 ID
- 只返回当前用户的任务列表（从数据库查询）
- 返回格式：JSON 数组

#### 2.4.3 任务状态查询模块（优先级：高）
**功能描述**: 查询单个任务的详细状态

**前端需求**:
- 任务详情页面或弹窗
- 显示任务的所有参数
- 显示任务当前状态
- 显示任务创建时间、更新时间
- 显示进度信息（如果可获取）

**后端需求**:
- REST API 端点：`GET /api/tasks/{task_id}`（需要 Token 验证）
- 从 Token 中获取用户 ID
- 从数据库查找任务，验证任务属于当前用户
- 通过 SSH 检查远程任务进程状态（简单检查进程是否存在）
- 如果任务完成，自动将模型添加到用户的模型列表中
- 返回任务详细信息

#### 2.4.4 任务日志查看模块（优先级：中）
**功能描述**: 查看任务的执行日志

**前端需求**:
- 日志查看页面或弹窗
- 显示日志内容（文本格式）
- 支持自动刷新（可选，轮询获取最新日志）
- 日志滚动显示（保留最近 N 行）

**后端需求**:
- REST API 端点：`GET /api/tasks/{task_id}/logs`（需要 Token 验证）
- 从 Token 中获取用户 ID
- 验证任务属于当前用户
- 通过 SSH 读取远程日志文件
- 日志文件路径：`{output_dir}/trainer_log.jsonl` 或标准输出日志
- 返回日志内容（字符串或行数组）

### 2.5 辅助功能模块

#### 2.5.1 SSH 连接配置模块
**功能描述**: 配置 SSH 连接参数

**实现方式**:
- 后端配置文件（config.py 或环境变量）
- 配置项：
  - SSH 主机地址
  - SSH 端口（默认：22）
  - SSH 用户名
  - SSH 密码或密钥路径
  - 远程工作目录（LlamaFactory 安装路径）

**注意**: 最小实现可以使用硬编码配置，后续可通过环境变量或配置文件修改

#### 2.5.2 错误处理模块
**功能描述**: 基础错误处理和提示

**前端需求**:
- 错误提示（使用 Toast 或 Alert）
- 网络错误处理
- 表单验证错误显示

**后端需求**:
- 异常捕获和错误响应
- SSH 连接失败处理
- 任务执行失败标记
- 返回标准错误格式：`{"error": "错误信息"}`

## 3. 技术实现细节

### 3.1 前端架构

#### 3.1.1 项目结构（推荐）
```
frontend/
├── public/
├── src/
│   ├── components/          # 组件目录
│   │   ├── TaskForm.tsx     # 任务提交表单
│   │   ├── TaskList.tsx     # 任务列表
│   │   ├── TaskDetail.tsx   # 任务详情
│   │   ├── LogViewer.tsx    # 日志查看器
│   │   ├── LoginForm.tsx    # 登录表单
│   │   ├── RegisterForm.tsx # 注册表单
│   │   ├── DatasetList.tsx  # 数据集文件列表
│   │   ├── ModelList.tsx    # 模型权重列表
│   │   ├── ChatInterface.tsx # 对话界面
│   │   └── ProtectedRoute.tsx # 路由保护组件
│   ├── pages/               # 页面目录
│   │   ├── Login.tsx        # 登录页
│   │   ├── Register.tsx     # 注册页
│   │   ├── Home.tsx         # 首页（任务列表）
│   │   ├── SubmitTask.tsx   # 提交任务页
│   │   ├── Datasets.tsx     # 数据集管理页
│   │   ├── Models.tsx       # 模型管理页
│   │   └── Chat.tsx         # 对话页
│   ├── services/            # API 服务
│   │   ├── api.ts           # API 调用封装
│   │   └── auth.ts          # 认证相关服务
│   ├── hooks/               # React Hooks
│   │   └── useAuth.ts       # 认证状态管理
│   ├── types/               # TypeScript 类型定义
│   │   ├── task.ts          # 任务相关类型
│   │   ├── user.ts          # 用户相关类型
│   │   ├── file.ts          # 文件相关类型
│   │   └── chat.ts          # 对话相关类型
│   ├── utils/               # 工具函数
│   │   └── storage.ts       # localStorage 封装
│   ├── App.tsx
│   ├── main.tsx
│   └── vite.config.ts
├── package.json
└── tsconfig.json
```

#### 3.1.2 技术选型建议
- **UI 框架**: React 18+ 或 Vue 3
- **UI 组件库**: Ant Design 或 Element Plus（可选，最小实现可只用基础 HTML）
- **HTTP 客户端**: Axios 或 Fetch API
- **状态管理**: React Hooks / Vue Composition API（最小实现可不使用全局状态管理）
- **路由**: React Router 或 Vue Router（最小实现可单页应用）

#### 3.1.3 API 接口定义
```typescript
// ===== 用户认证 API =====
// 用户注册
POST /api/auth/register
Request Body: {
  username: string;
  email: string;
  password: string;
}
Response: {
  message: string;
}

// 用户登录
POST /api/auth/login
Request Body: {
  username_or_email: string;
  password: string;
  remember_me?: boolean;
}
Response: {
  access_token: string;
  token_type: string;
  user: {
    user_id: string;
    username: string;
    email: string;
  };
}

// 获取当前用户信息
GET /api/auth/me
Headers: { Authorization: "Bearer {token}" }
Response: {
  user_id: string;
  username: string;
  email: string;
}

// ===== 文件管理 API =====
// 上传数据集文件
POST /api/files/datasets
Headers: { Authorization: "Bearer {token}" }
Request: multipart/form-data
  - file: File
Response: {
  file_id: string;
  filename: string;
  file_path: string;
  size: number;
  message: string;
}

// 获取数据集文件列表
GET /api/files/datasets
Headers: { Authorization: "Bearer {token}" }
Response: {
  files: Array<{
    file_id: string;
    filename: string;
    file_path: string;
    size: number;
    created_at: string;
  }>;
}

// 删除数据集文件
DELETE /api/files/datasets/{file_id}
Headers: { Authorization: "Bearer {token}" }
Response: {
  message: string;
}

// 获取模型权重文件列表
GET /api/files/models
Headers: { Authorization: "Bearer {token}" }
Response: {
  models: Array<{
    model_id: string;
    name: string;
    model_path: string;
    task_id: string;
    size: number;
    created_at: string;
  }>;
}

// 获取可用模型列表（用于对话）
GET /api/models/available
Headers: { Authorization: "Bearer {token}" }
Response: {
  models: Array<{
    model_id: string;
    name: string;
    model_path: string;
  }>;
}

// ===== 任务管理 API =====
// 提交任务
POST /api/tasks
Headers: { Authorization: "Bearer {token}" }
Request Body: {
  name: string;
  model_name: string;
  dataset_path: string;
  epochs?: number;
  learning_rate?: number;
  batch_size?: number;
  output_dir?: string;
}
Response: {
  task_id: string;
  status: string;
  message: string;
}

// 获取任务列表
GET /api/tasks
Headers: { Authorization: "Bearer {token}" }
Response: {
  tasks: Array<{
    task_id: string;
    name: string;
    model_name: string;
    status: string;
    created_at: string;
    updated_at: string;
  }>;
}

// 获取任务详情
GET /api/tasks/{task_id}
Headers: { Authorization: "Bearer {token}" }
Response: {
  task_id: string;
  name: string;
  model_name: string;
  dataset_path: string;
  epochs: number;
  learning_rate: number;
  batch_size: number;
  output_dir: string;
  status: string;
  created_at: string;
  updated_at: string;
}

// 获取任务日志
GET /api/tasks/{task_id}/logs
Headers: { Authorization: "Bearer {token}" }
Response: {
  logs: string;  // 日志内容（文本）
}

// ===== 对话 API =====
// 发送消息
POST /api/chat/completion
Headers: { Authorization: "Bearer {token}" }
Request Body: {
  model_path: string;
  messages: Array<{
    role: "user" | "assistant" | "system";
    content: string;
  }>;
  temperature?: number;
  max_tokens?: number;
}
Response: {
  role: "assistant";
  content: string;
}
```

### 3.2 后端架构

#### 3.2.1 项目结构（推荐）
```
backend/
├── app/
│   ├── __init__.py
│   ├── main.py              # FastAPI 应用入口
│   ├── config.py            # 配置文件
│   ├── database.py          # 数据库连接和初始化
│   ├── models.py            # Pydantic 数据模型
│   ├── db_models.py         # SQLAlchemy 数据库模型
│   ├── dependencies.py      # 依赖注入（如 Token 验证）
│   ├── routers/             # 路由模块
│   │   ├── auth.py          # 认证相关路由
│   │   ├── tasks.py         # 任务相关路由
│   │   ├── files.py         # 文件管理路由
│   │   └── chat.py          # 对话路由
│   ├── services/            # 业务逻辑
│   │   ├── auth_service.py  # 认证服务
│   │   ├── task_service.py  # 任务服务
│   │   ├── file_service.py  # 文件服务
│   │   ├── chat_service.py  # 对话服务
│   │   └── ssh_service.py   # SSH 服务
│   └── utils/               # 工具函数
│       ├── security.py      # 密码加密、JWT Token
│       └── file_utils.py    # 文件处理工具
├── database.db              # SQLite 数据库文件（自动生成）
├── requirements.txt
└── .env.example             # 环境变量示例
```

#### 3.2.2 技术选型
- **Web 框架**: FastAPI（推荐，异步支持好，自动生成 API 文档）
- **数据库**: SQLite（最小实现）+ SQLAlchemy ORM
- **SSH 客户端**: Paramiko
- **数据验证**: Pydantic（FastAPI 自带）
- **认证**: python-jose（JWT Token）+ passlib（密码加密）
- **文件上传**: python-multipart（FastAPI 文件上传支持）
- **异步支持**: asyncio + Paramiko 的线程池执行

#### 3.2.3 核心代码结构

**数据模型（models.py - Pydantic）**:
```python
from pydantic import BaseModel, EmailStr
from typing import Optional
from datetime import datetime

# 用户相关模型
class UserRegister(BaseModel):
    username: str
    email: EmailStr
    password: str

class UserLogin(BaseModel):
    username_or_email: str
    password: str
    remember_me: Optional[bool] = False

class User(BaseModel):
    user_id: str
    username: str
    email: str

# 任务相关模型
class TaskCreate(BaseModel):
    name: str
    model_name: str
    dataset_path: str
    epochs: Optional[int] = 3
    learning_rate: Optional[float] = 5e-5
    batch_size: Optional[int] = 4
    output_dir: Optional[str] = None

class Task(BaseModel):
    task_id: str
    user_id: str
    name: str
    model_name: str
    dataset_path: str
    epochs: int
    learning_rate: float
    batch_size: int
    output_dir: str
    status: str  # pending, running, completed, failed
    created_at: datetime
    updated_at: datetime
    ssh_command: Optional[str] = None

# 文件相关模型
class DatasetFile(BaseModel):
    file_id: str
    user_id: str
    filename: str
    file_path: str
    size: int
    created_at: datetime

class ModelFile(BaseModel):
    model_id: str
    user_id: str
    name: str
    model_path: str
    task_id: Optional[str] = None
    size: Optional[int] = None
    created_at: datetime

# 对话相关模型
class ChatMessage(BaseModel):
    role: str  # "user" | "assistant" | "system"
    content: str

class ChatRequest(BaseModel):
    model_path: str
    messages: list[ChatMessage]
    temperature: Optional[float] = 0.7
    max_tokens: Optional[int] = 2048

class ChatResponse(BaseModel):
    role: str
    content: str
```

**数据库模型（db_models.py - SQLAlchemy）**:
```python
from sqlalchemy import Column, String, Integer, Float, DateTime, ForeignKey, Text
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import relationship
from datetime import datetime
import uuid

Base = declarative_base()

class UserDB(Base):
    __tablename__ = "users"
    user_id = Column(String, primary_key=True, default=lambda: str(uuid.uuid4()))
    username = Column(String, unique=True, nullable=False)
    email = Column(String, unique=True, nullable=False)
    hashed_password = Column(String, nullable=False)
    created_at = Column(DateTime, default=datetime.utcnow)

class TaskDB(Base):
    __tablename__ = "tasks"
    task_id = Column(String, primary_key=True, default=lambda: str(uuid.uuid4()))
    user_id = Column(String, ForeignKey("users.user_id"), nullable=False)
    name = Column(String, nullable=False)
    model_name = Column(String, nullable=False)
    dataset_path = Column(String, nullable=False)
    epochs = Column(Integer, default=3)
    learning_rate = Column(Float, default=5e-5)
    batch_size = Column(Integer, default=4)
    output_dir = Column(String, nullable=False)
    status = Column(String, default="pending")
    ssh_command = Column(Text, nullable=True)
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)

class DatasetFileDB(Base):
    __tablename__ = "dataset_files"
    file_id = Column(String, primary_key=True, default=lambda: str(uuid.uuid4()))
    user_id = Column(String, ForeignKey("users.user_id"), nullable=False)
    filename = Column(String, nullable=False)
    file_path = Column(String, nullable=False)
    size = Column(Integer, nullable=False)
    created_at = Column(DateTime, default=datetime.utcnow)

class ModelFileDB(Base):
    __tablename__ = "model_files"
    model_id = Column(String, primary_key=True, default=lambda: str(uuid.uuid4()))
    user_id = Column(String, ForeignKey("users.user_id"), nullable=False)
    name = Column(String, nullable=False)
    model_path = Column(String, nullable=False)
    task_id = Column(String, ForeignKey("tasks.task_id"), nullable=True)
    size = Column(Integer, nullable=True)
    created_at = Column(DateTime, default=datetime.utcnow)
```

**SSH 服务（ssh_service.py）**:
```python
import paramiko
import json
from typing import Optional, Tuple, Dict

class SSHService:
    def __init__(self, host, port, username, password=None, key_path=None):
        self.host = host
        self.port = port
        self.username = username
        self.password = password
        self.key_path = key_path
    
    def _get_client(self) -> paramiko.SSHClient:
        """获取 SSH 客户端连接"""
        client = paramiko.SSHClient()
        client.set_missing_host_key_policy(paramiko.AutoAddPolicy())
        if self.key_path:
            client.connect(
                hostname=self.host,
                port=self.port,
                username=self.username,
                key_filename=self.key_path
            )
        else:
            client.connect(
                hostname=self.host,
                port=self.port,
                username=self.username,
                password=self.password
            )
        return client
    
    def execute_command(self, command: str, background: bool = False, timeout: int = 30) -> Tuple[str, str, int]:
        """
        执行命令
        返回: (stdout, stderr, return_code)
        """
        client = self._get_client()
        try:
            if background:
                # 后台执行，立即返回
                stdin, stdout, stderr = client.exec_command(command)
                return "", "", 0
            else:
                stdin, stdout, stderr = client.exec_command(command, timeout=timeout)
                exit_status = stdout.channel.recv_exit_status()
                stdout_text = stdout.read().decode('utf-8')
                stderr_text = stderr.read().decode('utf-8')
                return stdout_text, stderr_text, exit_status
        finally:
            client.close()
    
    def execute_chat_script(self, config: Dict, script_path: str, timeout: int = 300) -> Dict:
        """
        执行对话推理脚本
        参数:
            config: 包含 base_model_path, adapter_path, messages, temperature, max_tokens 的字典
            script_path: 远程服务器上脚本的路径
            timeout: 超时时间（秒），默认 300 秒
        返回: 推理结果字典 {"role": "assistant", "content": "..."}
        """
        # 将配置序列化为 JSON
        config_json = json.dumps(config, ensure_ascii=False)
        # 构建命令（通过命令行参数传递 JSON）
        command = f"python3 {script_path} '{config_json}'"
        
        stdout, stderr, return_code = self.execute_command(command, background=False, timeout=timeout)
        
        if return_code != 0:
            raise Exception(f"推理脚本执行失败: {stderr}")
        
        # 解析 JSON 输出
        try:
            result = json.loads(stdout)
            return result
        except json.JSONDecodeError:
            raise Exception(f"解析推理结果失败: {stdout}")
    
    def check_process(self, process_id: str) -> bool:
        """
        检查进程是否运行
        """
        command = f"ps -p {process_id} > /dev/null 2>&1 && echo 'running' || echo 'stopped'"
        stdout, stderr, return_code = self.execute_command(command)
        return "running" in stdout
    
    def read_file(self, file_path: str) -> str:
        """
        读取远程文件内容
        """
        command = f"cat {file_path}"
        stdout, stderr, return_code = self.execute_command(command)
        if return_code != 0:
            raise Exception(f"读取文件失败: {stderr}")
        return stdout
    
    def upload_file(self, local_path: str, remote_path: str):
        """
        上传文件到远程服务器（使用 SFTP）
        """
        client = self._get_client()
        try:
            sftp = client.open_sftp()
            sftp.put(local_path, remote_path)
            sftp.close()
        finally:
            client.close()
```

**任务服务（task_service.py）**:
```python
from typing import List, Optional
from app.models import Task, TaskCreate

class TaskService:
    def create_task(self, task_data: TaskCreate) -> Task:
        """
        创建任务并启动执行
        """
        pass
    
    def get_task(self, task_id: str) -> Optional[Task]:
        """
        获取任务详情
        """
        pass
    
    def list_tasks(self) -> List[Task]:
        """
        获取所有任务
        """
        pass
    
    def get_task_logs(self, task_id: str) -> str:
        """
        获取任务日志
        """
        pass
    
    def build_training_command(self, task: Task) -> str:
        """
        构建 LlamaFactory 训练命令
        """
        pass
```

**对话服务（chat_service.py）**:
```python
from typing import Dict
from app.models import ChatRequest, ChatResponse
from app.services.ssh_service import SSHService

class ChatService:
    def __init__(self, ssh_service: SSHService):
        self.ssh_service = ssh_service
        self.chat_script_path = "/path/to/llamafactory/chat_inference.py"  # 从配置读取
    
    def chat_completion(self, request: ChatRequest, base_model_path: str) -> ChatResponse:
        """
        执行对话推理
        参数:
            request: 对话请求
            base_model_path: 基础模型路径（从模型信息中获取）
        返回: 对话响应
        """
        # 构建推理配置
        config = {
            "base_model_path": base_model_path,
            "adapter_path": request.model_path,
            "messages": [msg.dict() for msg in request.messages],
            "temperature": request.temperature,
            "max_tokens": request.max_tokens
        }
        
        # 执行推理脚本
        result = self.ssh_service.execute_chat_script(
            config=config,
            script_path=self.chat_script_path,
            timeout=300  # 5分钟超时
        )
        
        return ChatResponse(**result)
```

**数据库服务（database.py）**:
```python
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker
from app.db_models import Base

# SQLite 数据库
SQLALCHEMY_DATABASE_URL = "sqlite:///./database.db"

engine = create_engine(
    SQLALCHEMY_DATABASE_URL, connect_args={"check_same_thread": False}
)
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)

def init_db():
    """初始化数据库，创建表"""
    Base.metadata.create_all(bind=engine)

def get_db():
    """获取数据库会话"""
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()
```

**认证服务（auth_service.py）**:
```python
from datetime import datetime, timedelta
from jose import JWTError, jwt
from passlib.context import CryptContext
from app.utils.security import verify_password, get_password_hash

pwd_context = CryptContext(schemes=["bcrypt"], deprecated="auto")
SECRET_KEY = "your-secret-key"  # 生产环境应从环境变量读取
ALGORITHM = "HS256"
ACCESS_TOKEN_EXPIRE_MINUTES = 1440  # 24小时

def create_access_token(data: dict, expires_delta: Optional[timedelta] = None):
    """创建 JWT Token"""
    to_encode = data.copy()
    if expires_delta:
        expire = datetime.utcnow() + expires_delta
    else:
        expire = datetime.utcnow() + timedelta(minutes=ACCESS_TOKEN_EXPIRE_MINUTES)
    to_encode.update({"exp": expire})
    encoded_jwt = jwt.encode(to_encode, SECRET_KEY, algorithm=ALGORITHM)
    return encoded_jwt

def verify_token(token: str):
    """验证 Token"""
    try:
        payload = jwt.decode(token, SECRET_KEY, algorithms=[ALGORITHM])
        return payload
    except JWTError:
        return None
```

### 3.3 SSH 连接实现细节

#### 3.3.1 连接配置
- 使用 Paramiko 建立 SSH 连接
- 支持密码和密钥两种认证方式
- 连接复用（可选，最小实现可每次新建连接）

#### 3.3.2 命令执行
- 训练命令需要在后台执行（使用 nohup 或 screen）
- 示例命令：
  ```bash
  cd /path/to/llamafactory && \
  nohup llamafactory-cli train \
    --model_name_or_path {model_name} \
    --dataset {dataset_path} \
    --template default \
    --finetuning_type lora \
    --output_dir {output_dir} \
    --num_train_epochs {epochs} \
    --learning_rate {learning_rate} \
    --per_device_train_batch_size {batch_size} \
    --cutoff_len 1024 \
    --plot_loss > {output_dir}/train.log 2>&1 &
  ```
- 保存进程 ID 用于后续状态检查

#### 3.3.3 日志获取
- 读取训练日志文件（通常是 `{output_dir}/train.log` 或 `trainer_log.jsonl`）
- 如果文件不存在，返回空或提示信息

#### 3.3.4 状态检查
- 通过检查进程 ID 是否存在判断任务状态
- 简单实现：`ps -p {pid}` 命令检查
- 更完善的实现：解析日志判断训练是否完成

#### 3.3.5 对话推理执行
**功能描述**: 通过 SSH 执行 Python 脚本进行模型推理

**实现方式**:
- 将消息列表和参数序列化为 JSON 字符串
- 通过 SSH 执行 Python 脚本，将 JSON 作为标准输入或命令行参数传入
- Python 脚本使用 LlamaFactory 的 Python API 进行推理
- 脚本将推理结果输出到标准输出（JSON 格式）
- 后端读取标准输出并解析返回给前端

**Python 脚本示例**（需要在远程服务器上准备）:
```python
#!/usr/bin/env python3
import sys
import json
from llamafactory import ChatModel

def main():
    # 从标准输入或命令行参数读取配置
    if len(sys.argv) > 1:
        config = json.loads(sys.argv[1])
    else:
        config = json.load(sys.stdin)
    
    # 解析参数
    base_model_path = config.get("base_model_path")
    adapter_path = config.get("adapter_path")
    messages = config.get("messages", [])
    temperature = config.get("temperature", 0.7)
    max_tokens = config.get("max_tokens", 2048)
    
    # 初始化模型
    model = ChatModel(
        model_name_or_path=base_model_path,
        adapter_name_or_path=adapter_path,
        template="default"
    )
    
    # 执行推理
    response = model.chat(
        messages=messages,
        temperature=temperature,
        max_new_tokens=max_tokens
    )
    
    # 输出结果（JSON 格式）
    result = {
        "role": "assistant",
        "content": response
    }
    print(json.dumps(result, ensure_ascii=False))

if __name__ == "__main__":
    main()
```

**SSH 执行命令示例**:
```bash
# 方式1：通过命令行参数传递 JSON
python3 /path/to/chat_script.py '{"base_model_path":"...","adapter_path":"...","messages":[...],"temperature":0.7}'

# 方式2：通过标准输入传递 JSON（适合较大数据）
echo '{"base_model_path":"...","adapter_path":"...","messages":[...]}' | python3 /path/to/chat_script.py
```

**实现注意事项**:
1. **脚本位置**: 对话脚本需要预先部署到远程服务器的固定路径（如 `/path/to/llamafactory/chat_inference.py`）
2. **环境要求**: 确保远程服务器 Python 环境已安装 LlamaFactory 及相关依赖
3. **超时设置**: 设置合理的 SSH 命令执行超时时间（如 300 秒），避免长时间等待
4. **错误处理**: 脚本执行失败时，捕获错误信息并返回给前端
5. **模型加载优化**: 如果频繁调用，考虑在远程服务器上使用模型服务（不在最小实现范围内）

**性能考虑**:
- 每次对话都需要建立 SSH 连接和执行脚本，响应时间较长（可能数秒到数十秒）
- 不适合高并发场景（最小实现可接受）
- 模型加载时间可能较长（首次调用或模型切换时）

## 4. 部署和配置

### 4.1 环境要求
- **前端**: Node.js 16+
- **后端**: Python 3.8+
- **远程服务器**: 已安装 LlamaFactory 的 Linux 服务器

### 4.2 配置文件示例

**后端环境变量（.env）**:
```
# SSH 配置
SSH_HOST=your-server-ip
SSH_PORT=22
SSH_USERNAME=your-username
SSH_PASSWORD=your-password
# 或使用密钥
SSH_KEY_PATH=/path/to/private/key
REMOTE_WORK_DIR=/path/to/llamafactory

# 应用配置
SECRET_KEY=your-secret-key-here-change-in-production
ALGORITHM=HS256
ACCESS_TOKEN_EXPIRE_MINUTES=1440

# 文件存储配置
REMOTE_USER_DATA_DIR=/remote/path/users
MAX_UPLOAD_SIZE=104857600  # 100MB in bytes

# 对话推理配置
CHAT_SCRIPT_PATH=/path/to/llamafactory/chat_inference.py
CHAT_TIMEOUT=300  # 对话超时时间（秒）
```

### 4.3 启动步骤

**前端启动**:
```bash
cd frontend
npm install
npm run dev
```

**后端启动**:
```bash
cd backend
pip install -r requirements.txt
# 配置 .env 文件
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

**远程服务器准备（对话功能）**:
```bash
# 1. 在远程服务器上创建对话推理脚本 chat_inference.py
# 脚本内容参考 3.3.5 节的示例代码

# 2. 将脚本上传到指定路径（与 .env 中的 CHAT_SCRIPT_PATH 一致）
# 例如：/path/to/llamafactory/chat_inference.py

# 3. 确保脚本有执行权限
chmod +x /path/to/llamafactory/chat_inference.py

# 4. 测试脚本是否正常工作（可选）
python3 /path/to/llamafactory/chat_inference.py '{"base_model_path":"test","adapter_path":"test","messages":[{"role":"user","content":"hello"}]}'

# 注意：脚本需要能够正确导入 LlamaFactory 库
```

## 5. 最小实现检查清单

### 5.1 前端功能
- [ ] 用户注册页面和功能
- [ ] 用户登录页面和功能
- [ ] Token 管理和状态维护
- [ ] 路由保护（未登录跳转登录页）
- [ ] 数据集文件上传功能
- [ ] 数据集文件列表展示
- [ ] 模型权重文件列表展示
- [ ] 任务提交表单（必填字段：任务名、模型名、数据集选择）
- [ ] 表单提交到后端 API
- [ ] 任务列表展示（仅显示当前用户的任务）
- [ ] 任务状态显示（至少支持 pending/running/completed/failed）
- [ ] 任务详情查看
- [ ] 任务日志查看
- [ ] 对话页面（模型选择、消息发送、历史显示）
- [ ] 基础错误提示

### 5.2 后端功能
- [ ] FastAPI 应用搭建
- [ ] SQLite 数据库初始化和表创建
- [ ] CORS 配置（允许前端跨域）
- [ ] 用户注册 API（POST /api/auth/register）
- [ ] 用户登录 API（POST /api/auth/login）
- [ ] JWT Token 生成和验证
- [ ] Token 验证中间件/依赖
- [ ] 密码加密存储（bcrypt）
- [ ] 文件上传 API（POST /api/files/datasets）
- [ ] 数据集文件列表 API（GET /api/files/datasets）
- [ ] 模型文件列表 API（GET /api/files/models）
- [ ] 任务创建 API（POST /api/tasks，需要 Token）
- [ ] 任务列表 API（GET /api/tasks，仅返回当前用户的任务）
- [ ] 任务详情 API（GET /api/tasks/{task_id}，验证用户权限）
- [ ] 任务日志 API（GET /api/tasks/{task_id}/logs，验证用户权限）
- [ ] 训练完成后自动添加模型到模型列表
- [ ] 对话 API（POST /api/chat/completion）
- [ ] SSH 连接功能
- [ ] SSH 文件上传功能
- [ ] 命令执行功能（后台执行训练命令和推理命令）
- [ ] 数据库存储（用户、任务、文件信息）
- [ ] 基础错误处理

### 5.3 集成测试
- [ ] 能够注册新用户
- [ ] 能够登录并获取 Token
- [ ] Token 验证正常工作
- [ ] 能够上传数据集文件
- [ ] 能够查看自己的数据集文件列表
- [ ] 能够提交任务（使用上传的数据集）
- [ ] 能够查看自己的任务列表
- [ ] 能够查看任务详情
- [ ] 能够查看任务日志（至少能看到命令已执行）
- [ ] 训练完成后模型自动添加到模型列表
- [ ] 能够查看模型列表
- [ ] 能够选择模型进行对话
- [ ] 对话功能正常工作（能够发送消息并收到回复）
- [ ] SSH 连接正常
- [ ] SSH 文件上传正常
- [ ] 训练命令能够成功提交到远程服务器
- [ ] 用户数据隔离正常（用户A无法访问用户B的数据）

## 6. 已知限制和后续优化方向

### 6.1 最小实现的限制
1. **数据库**: 使用 SQLite，不支持并发写入（生产环境应使用 PostgreSQL）
2. **文件存储**: 文件直接存储在远程服务器，没有对象存储（如 S3）
3. **任务状态更新**: 需要手动刷新，没有自动轮询更新
4. **并发控制**: 没有任务队列，多个任务同时执行可能冲突
5. **错误恢复**: 没有任务重试机制
6. **安全性**: 
   - SSH 密码可能存储在配置文件中（生产环境应使用密钥和环境变量）
   - Token Secret Key 硬编码（生产环境应从环境变量读取）
   - 没有 HTTPS（生产环境必须使用）
7. **文件管理**: 
   - 没有文件大小限制检查（前端有提示，后端应验证）
   - 没有文件类型验证
   - 删除文件时不检查是否被任务使用
8. **资源监控**: 没有 GPU/CPU 使用率监控
9. **任务取消**: 没有任务取消功能
10. **对话功能**: 
    - 没有流式输出（一次性返回完整回复）
    - 没有对话历史持久化（仅保存在前端）
    - 没有多轮对话上下文管理优化

### 6.2 后续优化方向（不在最小实现范围内）
1. 使用 PostgreSQL 替代 SQLite（支持更好的并发）
2. WebSocket 实时推送任务状态更新
3. 任务队列系统（Celery + Redis）
4. 更完善的用户权限系统（管理员、普通用户等）
5. 对象存储集成（S3/MinIO）用于文件存储
6. 文件版本管理
7. 模型下载功能
8. 训练进度可视化（损失曲线等）
9. 任务模板功能
10. 资源监控和告警
11. 任务日志实时流式输出
12. 对话流式输出（Server-Sent Events 或 WebSocket）
13. 对话历史持久化（数据库存储）
14. 多服务器支持
15. 模型版本管理
16. 数据集预处理功能
17. 批量任务提交
18. 任务调度和优先级管理
19. 用户配额管理（存储空间、任务数量限制）
20. 邮件通知（任务完成通知等）

## 7. 参考资料

- LlamaFactory 官方文档: https://llamafactory.readthedocs.io/
- LlamaFactory GitHub: https://github.com/hiyouga/LLaMA-Factory
- FastAPI 文档: https://fastapi.tiangolo.com/
- Paramiko 文档: https://www.paramiko.org/
- Vite 文档: https://vitejs.dev/

## 8. 开发优先级

### Phase 1: 用户认证和基础功能（最小可用版本）
1. 数据库设计和初始化
2. 用户注册和登录功能
3. JWT Token 认证
4. 前端登录/注册页面
5. 路由保护和状态管理

### Phase 2: 文件管理功能
1. 数据集文件上传功能
2. 数据集文件列表和管理
3. 模型权重文件列表（从任务自动添加）
4. 前端文件管理页面

### Phase 3: 任务管理功能
1. 后端 SSH 连接和命令执行
2. 后端任务创建 API（关联用户和数据集）
3. 前端任务提交表单（集成数据集选择）
4. 前端任务列表展示（用户隔离）
5. 任务详情查看
6. 任务日志查看
7. 训练完成后自动添加模型

### Phase 4: 对话功能
1. 模型选择功能
2. 对话 API 实现（SSH 执行推理命令）
3. 前端对话界面
4. 消息发送和显示

### Phase 5: 测试和优化
1. 基础测试验证完整流程跑通
2. 用户数据隔离测试
3. 错误处理完善
4. UI 美化
5. 加载状态提示
6. 更好的错误提示

---

**文档版本**: v2.0  
**创建日期**: 2024  
**最后更新**: 2024

## 9. 更新日志

### v2.0 (2024)
- 添加用户认证模块（注册、登录、Token 管理）
- 添加文件存储模块（数据集文件、模型权重文件）
- 添加对话模块（模型选择、对话交互）
- 更新数据存储方案（从内存存储改为 SQLite 数据库）
- 添加用户数据隔离需求
- 更新 API 接口定义
- 更新项目结构和代码示例
- 更新开发优先级和检查清单

### v1.0 (2024)
- 初始版本
- 基础任务管理功能
- SSH 连接和命令执行
- 内存存储方案

